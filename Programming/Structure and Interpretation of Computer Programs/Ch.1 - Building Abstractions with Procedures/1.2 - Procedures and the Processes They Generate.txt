">>" my attempt to the Exercise
"!" after peeking answer :) from http://community.schemewiki.org/?sicp-solutions

Comment:
    1. Recursive process: a procedure that with operations that first expand then contract. The additional part is 
    "deferred operations" where the interpreter need to keep track.
    2. Iterative process: a procedure that does not grow or shrink. Interpreter can easily go back to different state,
    given the proper parameter.
    3. A recursive procedure is a procedure that call itself, but this doens't necessary make it a recursive process if state
    is not kept by interpreter
    4. order of growth: gross measure of the resources required by a process as the inputs become larger,
    the famous big O notation.
        having logarithmic growth seems to be the sweetspot, when the input double, the resource requirement is constant


Exercise 1.9. Each of the following two procedures defines a method for adding two positive integers in
terms of the procedures inc, which increments its argument by 1, and dec, which decrements its
argument by 1.
(define (+ a b)
    (if (= a 0)
        b
        (inc (+ (dec a) b))))

(define (+ a b)
(if (= a 0)
b
(+ (dec a) (inc b))))
Using the substitution model, illustrate the process generated by each procedure in evaluating (+ 4 5).
Are these processes iterative or recursive?

>> first one is a recurisve process, variable a or b cannot be keept track to excute the processs, 
also it is an expansive process, with info stored in layer of "inc".
(define (+ a b)
    (if (= a 0)
        b
        (inc (+ (dec a) b))))

inc(+ 3 5)
inc(inc(+ 2 5))
inc(inc(inc(+ 1 5)))
inc(inc(inc(5)))
inc(inc(6))
inc(7)
8

>> second one is iterative, as the state is tracked by variable a,b. Even though 
the syntax is also recursive by calling itself. it isn't an expansive process.
(define (+ a b)
(if (= a 0)
b
(+ (dec a) (inc b))))

(+ 3 6) 
(+ 2 7)
(+ 1 8)
(+ 0 9)
(5)

! correct~ the easy way to notice is the first one call itself as sub procedure, while second one call itself at the top

Exercise 1.10. The following procedure computes a mathematical function called Ackermann's function.
(define (A x y)
    (cond ((= y 0) 0)
        ((= x 0) (* 2 y))
        ((= y 1) 2)
        (else (A (- x 1)
                 (A x (- y 1))))))

What are the values of the following expressions?
(A 1 10)

>> (A 0 A(1, 9))
>> (A 0 A(0, A(1, 8)))
>> so forth, until the end it reach ....A(0, A(1, 1))
>> A(1, 1) return 2 , all previously x==0 return 2 * y.
>> so it is 2 square of itself by 10 times, so return 1024

(A 2 4)
>> (A 1, (A 2, 3))
>> repeat 4 times, until .......A( 1 (A 2, 1))
>> (A 1, 16)
>> (A 0, A(1, 15))
>> repeat 16 times, until .......A(0 (A 1, 1))
>> so it is 2 power 16 times, so return 65536 

(A 3 3)
>> (A 2, A(3, 2))
>> (A 2, A (2, A(3, 1)))
>> (A 2, A (2, 2))
>> (A 2, A (1, A(2, 1)))
>> (A 2, A (1, 2))
>> (A 2, A (0, A(1, 1)))
>> (A 2, A (0, 2))
>> (A 2, 4)
>> same as above return 65536

! correct for all 3 above

Give concise mathematical definitions for the functions computed by the procedures f, g, and h for
positive integer values of n. For example, (k n) computes 5n2.
Consider the following procedures, where A is the procedure defined above:
(define (f n) (A 0 n))
(define (g n) (A 1 n))
(define (h n) (A 2 n))
(define (k n) (* 5 n n))

>> (f n) (A 0 n)computes 2n
>> (g n) (A 1 n)computes 2 power of n
>> (h n) (A 2 n) computes 2 (power of 2 power of (n))

Exercise 1.11. A function f is defined by the rule that f(n) = n if n<3 and f(n) = f(n - 1) + 2f(n - 2) + 3f(n -
3) if n> 3. Write a procedure that computes f by means of a recursive process. Write a procedure that
computes f by means of an iterative process.


# recursive is just the translation of the quesiton, can't quite get how the iterative function though
def func(n):
    if n < 3:
        return n
    elif n >=3:
        a = func(n-1) 
        b = 2*func(n-2)
        c = 3*func(n-3)
        return func(n-1) + 2*func(n-2) + 3*func(n-3)

Exercise 1.12. The following pattern of numbers is called Pascal's triangle.
The numbers at the edge of the triangle are all 1, and each number inside the triangle is the sum of the two
numbers above it. Write a procedure that computes elements of Pascal's triangle by means of a recursive
process.

>> after searching a bit on google and stackoverflow, 
>> I did think of needing to refer to previous row, but couldn't really get to the point on the part of calling it recursively :(
def pascal(n):
    if n == 0:
        return [1]
    elif n == 1:
        return [1, 1]
    else:
        new_row = [1]
        last_row = pascal(n-1)
        for i in range(len(last_row)-1):
            new_row.append(last_row[i] + last_row[i+1])
        new_row += [1]
    return new_row

>> Exercise 1.13, this is just plain too hard, skiping this!
>> luckily I am not mathematican!

Exercise 1.14. Draw the tree illustrating the process generated by the count-change procedure of
section 1.2.2 in making change for 11 cents. What are the orders of growth of the space and number of
steps used by this process as the amount to be changed increases?

>> tried to draw the chart, it seems every call of function "cc" will call
>> itself twice, so number of steps grow expotentially at power of two.
>> while growth of space should be linear as the fibonacci function
>> as one need only to keep track of the nodes above.


Exercise 1.15. The sine of an angle (specified in radians) can be computed by making use of the
approximation sin x x if x is sufficiently small, and the trigonometric identity
to reduce the size of the argument of sin. (For purposes of this exercise an angle is considered
``sufficiently small'' if its magnitude is not greater than 0.1 radians.) These ideas are incorporated in the
following procedures:
(define (cube x) (* x x x))
(define (p x) (- (* 3 x) (* 4 (cube x))))
(define (sine angle)
(if (not (> (abs angle) 0.1))
angle
(p (sine (/ angle 3.0)))))
a. How many times is the procedure p applied when (sine 12.15) is evaluated?
>> it takes 5 times to get 12.5/3 in function "sine" to get below 0.1 radians

b. What is the order of growth in space and number of steps (as a function of a) used by the process
generated by the sine procedure when (sine a) is evaluated?
>> number of space is constant, as there is no additional part after calling sine itself
>> number of steps is linear, the higher the angle, the more steps/ compute resource required
>> this is a iternative process

! it kinds of looks like iternative process is "better" than recursive one? at least resource-wise

Exercise 1.16. Design a procedure that evolves an iterative exponentiation process that uses successive
squaring and uses a logarithmic number of steps, as does fast-expt. (Hint: Using the observation that
(bn/2)2 = (b2)n/2, keep, along with the exponent n and the base b, an additional state variable a, and define
the state transformation in such a way that the product a bn is unchanged from state to state. At the
beginning of the process a is taken to be 1, and the answer is given by the value of a at the end of the
process. In general, the technique of defining an invariant quantity that remains unchanged from state to
state is a powerful way to think about the design of iterative algorithms.)

!total no idea on this one
!(define (fast-expt b n)
  (define (iter a b n)
    (cond ((= n 0) a)
          ((even? n) (iter a (square b) (/ n 2)))
          (else (iter (* a b) b (- n 1)))))


Exercise 1.17. The exponentiation algorithms in this section are based on performing exponentiation by
means of repeated multiplication. In a similar way, one can perform integer multiplication by means of
repeated addition. The following multiplication procedure (in which it is assumed that our language can
only add, not multiply) is analogous to the expt procedure:
(define (* a b)
(if (= b 0)
0
(+ a (* a (- b 1)))))
This algorithm takes a number of steps that is linear in b. Now suppose we include, together with addition,
operations double, which doubles an integer, and halve, which divides an (even) integer by 2. Using
these, design a multiplication procedure analogous to fast-expt that uses a logarithmic number of
steps.
>> a is like base number, while b is the number of steps/times to add a itself. So basically a * b is the result.
define (log* a b)
  (cond (= b 0 ) 0)
  (even? b) (* (double a) (halve b))
            (* (double a) (halve b-1) + a)
   
>> This can get to the result in 1 step, so isn't log function
! Almost get this one right, the else clause should be calling it self as such: 
!define (* a b)
  (cond (= b 0 ) 0)
  (even? b) (double (* a (halve b)))
            (a + (* a b-1)

            
Exercise 1.18. Using the results of exercises 1.16 and 1.17, devise a procedure that generates an iterative
process for multiplying two integers in terms of adding, doubling, and halving and uses a logarithmic
number of steps.

! (define (* a b) 
   (define (iter accumulator a b) 
     (cond ((= b 0) accumulator) 
           ((even? b) (iter accumulator (double a) (halve b))) 
           (else (iter (+ accumulator a) a (- b 1))))) 


Exercise 1.19. There is a clever algorithm for computing the Fibonacci numbers in a logarithmic number
of steps. Recall the transformation of the state variables a and b in the fib-iter process of
section 1.2.2: a
 a + b and b
 a. Call this transformation T, and observe that applying T over and over
again n times, starting with 1 and 0, produces the pair Fib(n + 1) and Fib(n). In other words, the Fibonacci
numbers are produced by applying Tn, the nth power of the transformation T, starting with the pair (1,0).
Now consider T to be the special case of p = 0 and q = 1 in a family of transformations Tpq, where Tpq
transforms the pair (a,b) according to a
 bq + aq + ap and b
 bp + aq. Show that if we apply such a
transformation Tpq twice, the effect is the same as using a single transformation Tp'q' of the same form, and
compute p' and q' in terms of p and q. This gives us an explicit way to square these transformations, and
thus we can compute Tn using successive squaring, as in the fast-expt procedure. Put this all together
to complete the following procedure, which runs in a logarithmic number of steps:41
(define (fib n)
(fib-iter 1 0 0 1 n))
(define (fib-iter a b p q count)
(cond ((= count 0) b)
((even? count)
(fib-iter a
b
<??>
 ; compute p'
<??>
 ; compute q'
(/ count 2)))
(else (fib-iter (+ (* b q) (* a q) (* a p))
(+ (* b p) (* a q))
p
q
(- count 1)))))

1.21
Use the smallest-divisor procedure to find the smallest divisor of each of the
following numbers: 199, 1999, 19999.
>> once again turn the smallest-divisor procedure into python
>> funny that 19999 have a divisor that isn't itself.

def smallest_divisor(n, test_divisor=2):
    if test_divisor ** 2 > n:
        return n
    elif n % test_divisor == 0:
        return test_divisor
    else:
        test_divisor += 1
        return smallest_divisor(n, test_divisor)

print(smallest_divisor(199)) # 199
print(smallest_divisor(1999)) # 1999
print(smallest_divisor(19999)) # 7


Exercise 1.22. Most Lisp implementations include a primitive called runtime that returns an integer
that specifies the amount of time the system has been running (measured, for example, in microseconds).
The following timed-prime-test procedure, when called with an integer n, prints n and checks to
see if n is prime. If n is prime, the procedure prints three asterisks followed by the amount of time used in
performing the test.
(define (timed-prime-test n)
    (newline)
    (display n)
    (start-prime-test n (runtime)))
(define (start-prime-test n start-time)
    (if (prime? n)
        (report-prime (- (runtime) start-time))))
(define (report-prime elapsed-time)
    (display " *** ")
    (display elapsed-time))
Using this procedure, write a procedure search-for-primes that checks the primality of consecutive
odd integers in a specified range. Use your procedure to find the three smallest primes larger than 1000;
larger than 10,000; larger than 100,000; larger than 1,000,000. Note the time needed to test each prime.
Since the testing algorithm has order of growth of ( n), you should expect that testing for primes
around 10,000 should take about
 10 times as long as testing for primes around 1000. Do your timing
data bear this out? How well do the data for 100,000 and 1,000,000 support the n prediction? Is your
result compatible with the notion that programs on your machine run in time proportional to the number of
steps required for the computation?

import time
from pprint import pprint
# exercise 1.22
def is_prime(n):
    return True if n == smallest_divisor(n) else False

def time_prime_test(n, quiet=True): 
    if not quiet:
        print('')
        print(n)
    start_time = time.time()
    if is_prime(n):
        elapsed_time = time.time() - start_time 
        if not quiet:
            print(' *** ')
            print(elapsed_time)
        return elapsed_time


# find the three smallest primes larger than 1000;
# larger than 10,000; 
# larger than 100,000; 
# larger than 1,000,000.

rng = [1000, 10000, 100000, 1000000]
result={}
for num in rng:
    start_num = num
    prime_lst = []
    while len(prime_lst) <3:
        elapsed_time = time_prime_test(num)
        if elapsed_time:
            prime_lst.append((num, elapsed_time))
        num+=1
    result[start_num] = prime_lst

pprint(result)

>> it doesn't seems to follow the algorithm that n increase by n time,
>> where compute time reduce by square root of n 


Exercise 1.23. The smallest-divisor procedure shown at the start of this section does lots of
needless testing: After it checks to see if the number is divisible by 2 there is no point in checking to see if
it is divisible by any larger even numbers. This suggests that the values used for test-divisor should
not be 2, 3, 4, 5, 6, ..., but rather 2, 3, 5, 7, 9, .... To implement this change, define a procedure next
that returns 3 if its input is equal to 2 and otherwise returns its input plus 2. Modify the smallest-
divisor procedure to use (next test-divisor) instead of (+ test-divisor 1). With
timed-prime-test incorporating this modified version of smallest-divisor, run the test for
each of the 12 primes found in exercise 1.22. Since this modification halves the number of test steps, you
should expect it to run about twice as fast. Is this expectation confirmed? If not, what is the observed ratio
of the speeds of the two algorithms, and how do you explain the fact that it is different from 2?

def next_divisor(n):
    if n == 2:
        return 3
    else:
        return n + 2

def modified_smallest_divisor(n, test_divisor=2):
    if test_divisor ** 2 > n:
        return n
    elif n % test_divisor == 0:
        return test_divisor
    else:
        test_divisor = next_divisor(test_divisor)
        return modified_smallest_divisor(n, test_divisor)

def modified_is_prime(n):
    return True if n == modified_smallest_divisor(n) else False

rng = [1000, 10000, 100000, 1000000]
result={}
for num in rng:
    start_num = num
    prime_lst = []
    while len(prime_lst) <3:
        elapsed_time = time_prime_test(num)
        if elapsed_time:
            prime_lst.append((num, elapsed_time))
        num+=1
    result[start_num] = prime_lst

result_1= [x for _,x in result[100000]]
result_2 = [x for _,x in result[1000000]]
[(y-x)/x*100 for x, y in zip(result_1, result_2)]

>> somehow I can get the new way 300% faster than the old one...
>> should be expecting it to be 100% faster..not sure why


Exercise 1.24. Modify the timed-prime-test procedure of exercise 1.22 to use fast-prime? (the
Fermat method), and test each of the 12 primes you found in that exercise. Since the Fermat test has
(log n) growth, how would you expect the time to test primes near 1,000,000 to compare with the time
needed to test primes near 1000? Do your data bear this out? Can you explain any discrepancy you find?

>> couldn't quite get this part to work..

def expmod(base, exp, m):
    if exp == 0:
        return 1
    elif exp % 2 == 0:
        return (
            (expmod(base, exp/2, m))**2
            % m
        )
    else:
        return (
            (base * (expmod(base, exp-1, m)))
            % m 
            )

from random import randrange
def fermat_test(n):
    def try_it(a):
        return a == expmod(a, n, n)
    print(randrange(1, n-1))
    try_it(1 + randrange(1, n-1))

def fast_prime(n, times):
    if times == 0: 
        return True
    elif fermat_test(n):
        return fast_prime(n, times-1)
    else:
        return False


Exercise 1.25. Alyssa P. Hacker complains that we went to a lot of extra work in writing expmod. After
all, she says, since we already know how to compute exponentials, we could have simply written
(define (expmod base exp m)
    (remainder (fast-expt base exp) m))

>> because even and and odd number treat "exp" 
    parameter differently when calling it recurivesly.

Exercise 1.26. Louis Reasoner is having great difficulty doing exercise 1.24. His fast-prime? test
seems to run more slowly than his prime? test. Louis calls his friend Eva Lu Ator over to help. When
they examine Louis's code, they find that he has rewritten the expmod procedure to use an explicit
multiplication, rather than calling square:
(define (expmod base exp m)
    (cond ((= exp 0) 1)
    ((even? exp)
        (remainder (* (expmod base (/ exp 2) m)
        (expmod base (/ exp 2) m))
        m))
    (else
        (remainder (* base (expmod base (- exp 1) m))
        m))))
``I don't see what difference that could make,'' says Louis. ``I do.'' says Eva. ``By writing the procedure
like that, you have transformed the (log n) process into a (n) process.'' Explain.

>> Lisp is applicative order, by NOT using square(), the section "(expmod base (/ exp 2) m)" 
   will need to be evaluated twice (recurisvely!!!). 


Exercise 1.27, and 1.28 is just too math heavy, skipping these for later :)


