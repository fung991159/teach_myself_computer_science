Intel-compatible processors also support an “extended-precision” floating-point
format with an 80-bit word divided into a sign bit, k = 15 exponent bits, a single
integer bit, and n = 63 fraction bits. The integer bit is an explicit copy of the
implied bit in the IEEE floating-point representation. That is, it equals 1 for
normalized values and 0 for denormalized values. Fill in the following table giving
the approximate values of some “interesting” numbers in this format:

s, k=15, integer bit, n=63
Smallest positive denormalized 
Bias = 2 ^ (14) - 1 = 16383
E = 1 - Bias
M= f = 2 ^ -63
V = 2 ^ (1-Bias) * 2 ^ -63 = 2^ (1-Bias - 63)


Largest normalized
e = 2^15 - 1
E = e - Bias 
M = 1 + f = 1 + 2^-63

V = 2 ^ 



| Description                    | Value            | Decimal          |
| ------------------------------ | ---------------- | ---------------- |
| Smallest positive denormalized | 0 00(15) 00(62)1 | 2^ (1-Bias - 63) |
| Smallest positive normalized   | 00(14)110(63)    | 2^(1-bias)       |
| Largest normalized             | 0 1(14)0 1 1(63) |

This format can be used in C programs compiled for Intel-compatible ma-
chines by declaring the data to be of type long double. However, it forces the
compiler to generate code based on the legacy 8087 floating-point instructions.
The resulting program will most likely run much slower than would be the case
for data type float or double.

> can't really get thwat is "Largest normalized", the online solution doesn't
> seems to make too much sense